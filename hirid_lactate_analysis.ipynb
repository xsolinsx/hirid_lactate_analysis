{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datetime\r\n",
    "import json\r\n",
    "import os\r\n",
    "import sqlite3\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import plotly.graph_objects as go\r\n",
    "from IPython.display import display\r\n",
    "from plotly.subplots import make_subplots\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Demographics and Dataset_Part-Patient_id bindings\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "BASE_DATASETS_PATH = \"D:/Data/Thesis_datasets\"\r\n",
    "DB_PATH = f\"{BASE_DATASETS_PATH}/raw_data.db\"\r\n",
    "RAW_OBS_PATH = f\"{BASE_DATASETS_PATH}/raw_stage/observation_tables\"\r\n",
    "lactate_var_ids = (\"24000524\", \"24000732\", \"24000485\")\r\n",
    "\r\n",
    "df_demo = pd.read_csv(\r\n",
    "    f\"{BASE_DATASETS_PATH}/reference_data/general_table.csv\",\r\n",
    "    dtype={\r\n",
    "        \"patientid\": int,\r\n",
    "        \"sex\": str,\r\n",
    "        \"age\": int,\r\n",
    "        \"discharge_status\": str,\r\n",
    "    },\r\n",
    "    parse_dates=[\"admissiontime\"],\r\n",
    ")\r\n",
    "display(df_demo.describe(include=\"all\", datetime_is_numeric=True))\r\n",
    "\r\n",
    "df_pid_part = pd.read_csv(\r\n",
    "    f\"{RAW_OBS_PATH}/observation_tables_index.csv\",\r\n",
    "    dtype={\r\n",
    "        \"patientid\": int,\r\n",
    "        \"part\": int,\r\n",
    "    },\r\n",
    ")\r\n",
    "df_pid_part.sort_values(by=[\"part\"], inplace=True)\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Data/Thesis_datasets/reference_data/general_table.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-db353ecad52c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;34m\"discharge_status\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     },\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"admissiontime\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_demo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime_is_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Data/Thesis_datasets/reference_data/general_table.csv'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Move Everything to SQLite\r\n",
    "Skip if you already have the DB\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for x in df_pid_part[\"part\"].unique():\r\n",
    "    if (x + 1) % 25 == 0:\r\n",
    "        print((x + 1) / len(df_pid_part[\"part\"].unique()))\r\n",
    "    df_part_X = pd.read_csv(\r\n",
    "        f\"{RAW_OBS_PATH}/csv/part-{x}.csv\",\r\n",
    "        dtype={\r\n",
    "            \"value\": str,\r\n",
    "            \"patientid\": int,\r\n",
    "            \"status\": int,\r\n",
    "            \"stringvalue\": str,\r\n",
    "            \"type\": str,\r\n",
    "            \"value\": str,\r\n",
    "            \"variableid\": str,\r\n",
    "        },\r\n",
    "        parse_dates=[\"datetime\", \"entertime\"],\r\n",
    "    )\r\n",
    "    conn = sqlite3.connect(DB_PATH)\r\n",
    "    df_part_X.to_sql(name=\"raw_data\", con=conn, if_exists=\"append\", index=False)\r\n",
    "    conn.close()\r\n",
    "\r\n",
    "conn = sqlite3.connect(DB_PATH)\r\n",
    "cur = conn.cursor()\r\n",
    "print(\"Creating index on datetime...\")\r\n",
    "cur.execute(\"CREATE INDEX index_datetime ON raw_data(datetime)\")\r\n",
    "print(\"Creating index on patientid...\")\r\n",
    "cur.execute(\"CREATE INDEX index_patient_id ON raw_data(patientid)\")\r\n",
    "print(\"Creating index on variableid...\")\r\n",
    "cur.execute(\"CREATE INDEX index_variable_id ON raw_data(variableid)\")\r\n",
    "conn.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filtering\r\n",
    "Skip if you already have the .txt filter files\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter Age >= 18 Years Old\r\n",
    "\r\n",
    "No effect, patients inside HiRID are all >= 18 years old\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "df_demo = df_demo[df_demo[\"age\"] >= 18]\r\n",
    "df_demo.reset_index(drop=True, inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter Stay >= 1 Day\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "filtered_pids = list()\r\n",
    "\r\n",
    "conn = sqlite3.connect(DB_PATH)\r\n",
    "df_raw = pd.read_sql(\r\n",
    "    \"SELECT patientid, MAX(datetime) as last_datetime FROM raw_data GROUP BY patientid\",\r\n",
    "    conn,\r\n",
    "    parse_dates=[\"last_datetime\"],\r\n",
    "    chunksize=1000,\r\n",
    ")\r\n",
    "for df in df_raw:\r\n",
    "    for tup in df.itertuples():\r\n",
    "        # last datetime - admissiontime\r\n",
    "        if (\r\n",
    "            tup.last_datetime\r\n",
    "            - df_demo[df_demo[\"patientid\"] == tup.patientid].iloc[0][\"admissiontime\"]\r\n",
    "        ) >= datetime.timedelta(days=1):\r\n",
    "            filtered_pids.append(tup.patientid)\r\n",
    "    print(len(filtered_pids))\r\n",
    "conn.close()\r\n",
    "\r\n",
    "with open(\"./results/patients_18+_1d+.txt\", \"w\") as f:\r\n",
    "    for x in filtered_pids:\r\n",
    "        f.write(f\"{x}\\n\")\r\n",
    "print(len(filtered_pids))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "511\n",
      "991\n",
      "1495\n",
      "1988\n",
      "2503\n",
      "3003\n",
      "3503\n",
      "4000\n",
      "4509\n",
      "4999\n",
      "5525\n",
      "6013\n",
      "6514\n",
      "7020\n",
      "7513\n",
      "8000\n",
      "8479\n",
      "8979\n",
      "9492\n",
      "9994\n",
      "10470\n",
      "10995\n",
      "11479\n",
      "11983\n",
      "12472\n",
      "12962\n",
      "13446\n",
      "13919\n",
      "14413\n",
      "14901\n",
      "15366\n",
      "15853\n",
      "16339\n",
      "16787\n",
      "16787\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter Lactate_Measurements >= 2\r\n",
    "\r\n",
    "This should be useless as the next filter tests that lactate_measurements >= 2 in the first 2d (CHECK)\r\n",
    "\r\n",
    "So obviously if there are 2+ lactate measurements in the first 2d then there are of course 2+ lactate measurements in general\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "filtered_pids = list()\r\n",
    "previously_filtered = list()\r\n",
    "with open(\"./results/patients_18+_1d+.txt\", \"r\") as f:\r\n",
    "    previously_filtered.extend(int(x) for x in f.readlines())\r\n",
    "\r\n",
    "conn = sqlite3.connect(DB_PATH)\r\n",
    "lact_vars = \"', '\".join([str(x) for x in lactate_var_ids])\r\n",
    "pids_checks = \", \".join([str(x) for x in previously_filtered])\r\n",
    "df_raw = pd.read_sql(\r\n",
    "    f\"SELECT patientid, COUNT(*) as cnt_lactate_measurements FROM raw_data WHERE (variableid IN ('{lact_vars}')) AND (patientid IN ({pids_checks})) GROUP BY patientid\",\r\n",
    "    conn,\r\n",
    "    chunksize=1000,\r\n",
    ")\r\n",
    "for df in df_raw:\r\n",
    "    for tup in df.itertuples():\r\n",
    "        # 2+ lactate measurements\r\n",
    "        if tup.cnt_lactate_measurements >= 2:\r\n",
    "            filtered_pids.append(tup.patientid)\r\n",
    "    print(len(filtered_pids))\r\n",
    "conn.close()\r\n",
    "\r\n",
    "with open(\"./results/patients_18+_1d+_2+lactate_measurements.txt\", \"w\") as f:\r\n",
    "    for x in filtered_pids:\r\n",
    "        f.write(f\"{x}\\n\")\r\n",
    "print(len(filtered_pids))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "980\n",
      "1958\n",
      "2941\n",
      "3922\n",
      "4906\n",
      "5881\n",
      "6863\n",
      "7839\n",
      "8816\n",
      "9798\n",
      "10775\n",
      "11762\n",
      "12740\n",
      "13720\n",
      "14701\n",
      "15209\n",
      "15209\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter Lactate_Measurements >= 2 in First 2d of Stay\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "filtered_pids = list()\r\n",
    "previously_filtered = list()\r\n",
    "with open(\"./results/patients_18+_1d+_2+lactate_measurements.txt\", \"r\") as f:\r\n",
    "    previously_filtered.extend(int(x) for x in f.readlines())\r\n",
    "\r\n",
    "conn = sqlite3.connect(DB_PATH)\r\n",
    "lact_vars = \"', '\".join([str(x) for x in lactate_var_ids])\r\n",
    "for pid in previously_filtered:\r\n",
    "    post_2d = df_demo[df_demo[\"patientid\"] == pid].iloc[0][\r\n",
    "        \"admissiontime\"\r\n",
    "    ] + datetime.timedelta(days=2)\r\n",
    "    df_raw = pd.read_sql(\r\n",
    "        f\"SELECT patientid, COUNT(*) AS cnt_lactate_measurements FROM raw_data WHERE (variableid IN ('{lact_vars}')) AND (patientid={pid}) AND (datetime<='{post_2d}')\",\r\n",
    "        conn,\r\n",
    "    )\r\n",
    "    for tup in df_raw.itertuples():\r\n",
    "        # 2+ lactate measurements in first 2d\r\n",
    "        if tup.cnt_lactate_measurements >= 2:\r\n",
    "            filtered_pids.append(pid)\r\n",
    "    if len(filtered_pids) % 1000 == 0:\r\n",
    "        print(len(filtered_pids))\r\n",
    "conn.close()\r\n",
    "\r\n",
    "with open(\"./results/patients_18+_1d+_2+lactate_measurements_in_first2d.txt\", \"w\") as f:\r\n",
    "    for x in filtered_pids:\r\n",
    "        f.write(f\"{x}\\n\")\r\n",
    "print(len(filtered_pids))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "15046\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract 5 Patients\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "patientids = list()\r\n",
    "with open(\"./results/patients_18+_1d+_2+lactate_measurements_in_first2d.txt\", \"r\") as f:\r\n",
    "    patientids.extend(int(x) for x in f.readlines())\r\n",
    "# patientids = patientids[:5]\r\n",
    "patientids = [17786, 16503, 22946, 9465, 6586]\r\n",
    "# create a folder for each patient\r\n",
    "for pid in patientids:\r\n",
    "    os.makedirs(f\"./results/{pid}\", exist_ok=True)\r\n",
    "patientids\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[17786, 16503, 22946, 9465, 6586]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split HiRID in patients' datasets\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "var_bindings = dict()\r\n",
    "with open(\"./var_id_var_name_bindings.json\", \"r\") as f:\r\n",
    "    var_bindings = json.load(f)\r\n",
    "\r\n",
    "for pid in patientids:\r\n",
    "    os.makedirs(f\"./results/{pid}\", exist_ok=True)\r\n",
    "    var_ids_checks = \"', '\".join(var_bindings[\"dynamic\"].keys())\r\n",
    "    conn = sqlite3.connect(DB_PATH)\r\n",
    "    df_raw = pd.read_sql(\r\n",
    "        f\"SELECT * FROM raw_data WHERE (patientid={pid}) AND (variableid IN ('{var_ids_checks}')) ORDER BY datetime, entertime\",\r\n",
    "        conn,\r\n",
    "        parse_dates=[\"datetime\", \"entertime\"],\r\n",
    "    )\r\n",
    "    df_raw.to_csv(f\"./results/{pid}/{pid}_original.csv\")\r\n",
    "    conn.close()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "var_bindings = dict()\r\n",
    "with open(\"./var_id_var_name_bindings.json\", \"r\") as f:\r\n",
    "    var_bindings = json.load(f)\r\n",
    "\r\n",
    "static_columns = list(var_bindings[\"static\"].values())\r\n",
    "for tup in df_demo[df_demo[\"patientid\"].isin(patientids)].itertuples():\r\n",
    "    conn = sqlite3.connect(DB_PATH)\r\n",
    "    last_datetime = pd.read_sql(\r\n",
    "        f\"SELECT MAX(datetime) as last_datetime FROM raw_data WHERE patientid={tup.patientid}\",\r\n",
    "        conn,\r\n",
    "        parse_dates=[\"last_datetime\"],\r\n",
    "    ).iloc[0][\"last_datetime\"]\r\n",
    "    df_static_sample = pd.DataFrame(columns=static_columns)\r\n",
    "    df_static_sample.loc[0] = [\r\n",
    "        getattr(tup, x, None) for x in var_bindings[\"static\"].keys()\r\n",
    "    ]\r\n",
    "    # compute length of stay\r\n",
    "    df_static_sample.at[0, \"Length of stay (days)\"] = round(\r\n",
    "        (last_datetime - df_static_sample.iloc[0][\"Admission Time\"]).total_seconds()\r\n",
    "        / 60\r\n",
    "        / 60\r\n",
    "        / 24,\r\n",
    "        2,\r\n",
    "    )\r\n",
    "    df_static_sample.to_csv(f\"./results/{tup.patientid}/{tup.patientid}_static.csv\")\r\n",
    "\r\n",
    "# avoid duplicated columns\r\n",
    "dynamic_columns = list(dict.fromkeys(var_bindings[\"dynamic\"].values()).keys())\r\n",
    "var_ids_checks = \"', '\".join(var_bindings[\"dynamic\"].keys())\r\n",
    "for pid in patientids:\r\n",
    "    conn = sqlite3.connect(DB_PATH)\r\n",
    "    df_pid_raw_data = pd.read_sql(\r\n",
    "        f\"SELECT * FROM raw_data WHERE (patientid={pid}) AND (variableid IN ('{var_ids_checks}')) ORDER BY datetime, entertime\",\r\n",
    "        conn,\r\n",
    "        parse_dates=[\"datetime\", \"entertime\"],\r\n",
    "    )\r\n",
    "    conn.close()\r\n",
    "    pid_admission_time = df_demo[df_demo[\"patientid\"] == pid][\"admissiontime\"].iloc[0]\r\n",
    "    tmp_dict = dict()\r\n",
    "    for tup in df_pid_raw_data.itertuples():\r\n",
    "        row_key = round((tup.datetime - pid_admission_time).total_seconds() / 60, 2)\r\n",
    "        if row_key not in tmp_dict:\r\n",
    "            tmp_dict[row_key] = dict.fromkeys(dynamic_columns)\r\n",
    "            tmp_dict[row_key][var_bindings[\"dynamic\"][\"datetime\"]] = tup.datetime\r\n",
    "            # entertime could be different for different values of the same record, maybe we should remove it\r\n",
    "            # i.e. some values have the same datetime (observation) but different entertime (insertion into db)\r\n",
    "            tmp_dict[row_key][var_bindings[\"dynamic\"][\"entertime\"]] = tup.entertime\r\n",
    "\r\n",
    "        if var_bindings[\"dynamic\"][tup.variableid] == \"Lactate\":\r\n",
    "            # check lactate variables priority (use arterial, if not available use venous)\r\n",
    "            if tup.variableid != \"24000524\":\r\n",
    "                # not arterial lactate, check if there's some lactate value already present\r\n",
    "                if (\r\n",
    "                    tmp_dict[row_key][var_bindings[\"dynamic\"][tup.variableid]]\r\n",
    "                    is not None\r\n",
    "                ):\r\n",
    "                    # lactate value already present, skip since it's probably an arterial one\r\n",
    "                    continue\r\n",
    "\r\n",
    "        tmp_dict[row_key][var_bindings[\"dynamic\"][tup.variableid]] = tup.value\r\n",
    "\r\n",
    "        # create new vars for bp\r\n",
    "        if var_bindings[\"dynamic\"][tup.variableid] in (\r\n",
    "            \"Invasive Diastolic Arterial Blood Pressure\",\r\n",
    "            \"Non-Invasive Diastolic Arterial Blood Pressure\",\r\n",
    "            \"Invasive Mean Arterial Blood Pressure\",\r\n",
    "            \"Non-Invasive Mean Arterial Blood Pressure\",\r\n",
    "            \"Invasive Systolic Arterial Blood Pressure\",\r\n",
    "            \"Non-Invasive Systolic Arterial Blood Pressure\",\r\n",
    "        ):\r\n",
    "            var_id = \"\"\r\n",
    "            if var_bindings[\"dynamic\"][tup.variableid] in (\r\n",
    "                \"Invasive Diastolic Arterial Blood Pressure\",\r\n",
    "                \"Non-Invasive Diastolic Arterial Blood Pressure\",\r\n",
    "            ):\r\n",
    "                var_id = \"diastolic_arterial_bp_computed\"\r\n",
    "            if var_bindings[\"dynamic\"][tup.variableid] in (\r\n",
    "                \"Invasive Mean Arterial Blood Pressure\",\r\n",
    "                \"Non-Invasive Mean Arterial Blood Pressure\",\r\n",
    "            ):\r\n",
    "                var_id = \"mean_arterial_bp_computed\"\r\n",
    "            if var_bindings[\"dynamic\"][tup.variableid] in (\r\n",
    "                \"Invasive Systolic Arterial Blood Pressure\",\r\n",
    "                \"Non-Invasive Systolic Arterial Blood Pressure\",\r\n",
    "            ):\r\n",
    "                var_id = \"systolic_arterial_bp_computed\"\r\n",
    "            # check bp variables priority (use invasive, if not available use non-invasive)\r\n",
    "            if tup.variableid not in (\"100\", \"110\", \"120\"):\r\n",
    "                # non-invasive bp, check if there's some bp value already present\r\n",
    "                if tmp_dict[row_key][var_bindings[\"dynamic\"][var_id]] is not None:\r\n",
    "                    # bp value already present, skip since it's probably an invasive one\r\n",
    "                    continue\r\n",
    "            tmp_dict[row_key][var_bindings[\"dynamic\"][var_id]] = tup.value\r\n",
    "\r\n",
    "    df_dynamic_sample = pd.DataFrame(\r\n",
    "        data=tmp_dict.values(),\r\n",
    "        columns=dynamic_columns,\r\n",
    "    )\r\n",
    "    df_dynamic_sample[\"Patient ID\"] = pid\r\n",
    "    df_dynamic_sample.set_index(\r\n",
    "        pd.Index(tmp_dict.keys(), name=\"Minutes of Stay\"),\r\n",
    "        drop=True,\r\n",
    "        inplace=True,\r\n",
    "    )\r\n",
    "    df_dynamic_sample.to_csv(f\"./results/{tup.patientid}/{tup.patientid}_dynamic.csv\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check Variables' Distributions\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "patientids = list()\r\n",
    "with open(\"./results/patients_18+_1d+_2+lactate_measurements_in_first2d.txt\", \"r\") as f:\r\n",
    "    patientids.extend(int(x) for x in f.readlines())\r\n",
    "\r\n",
    "var_bindings = dict()\r\n",
    "with open(\"./var_id_var_name_bindings.json\", \"r\") as f:\r\n",
    "    var_bindings = json.load(f)\r\n",
    "\r\n",
    "os.makedirs(\"./results/distribution_plots\", exist_ok=True)\r\n",
    "# avoid duplicated columns\r\n",
    "dynamic_columns = list(dict.fromkeys(var_bindings[\"dynamic\"].values()).keys())\r\n",
    "pids_checks = \"', '\".join([str(x) for x in patientids])\r\n",
    "# skip patientid, datetime, entertime\r\n",
    "for var in dynamic_columns[3:]:\r\n",
    "    # remove invalid characters from filename\r\n",
    "    filename = var\r\n",
    "    for c in (\"\\\\\", \"/\", \":\", \"*\", \"?\", '\"', \"<\", \">\", \"|\"):\r\n",
    "        filename = filename.replace(c, \"_\")\r\n",
    "    # get all HiRID var_ids connected to final variable name\r\n",
    "    var_X_ids = [k for k, v in var_bindings[\"dynamic\"].items() if v == var]\r\n",
    "    print(var, var_X_ids)\r\n",
    "    var_ids_checks = \"', '\".join(var_X_ids)\r\n",
    "    conn = sqlite3.connect(DB_PATH)\r\n",
    "    df_var_raw = pd.read_sql(\r\n",
    "        f\"SELECT value FROM raw_data WHERE (patientid IN ('{pids_checks}')) AND (variableid IN ('{var_ids_checks}'))\",\r\n",
    "        conn,\r\n",
    "    )\r\n",
    "    df_var_raw.to_csv(f\"./results/distribution_plots/{filename}.csv\")\r\n",
    "    conn.close()\r\n",
    "    df_var_raw = pd.read_csv(\r\n",
    "        f\"./results/distribution_plots/{filename}.csv\", index_col=0\r\n",
    "    )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run plots.R to get distribution plots\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c643c1ae580a7eadaaaf8eb2846b79a7c1c6576dfd78895ee4877fd48cad0027"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}