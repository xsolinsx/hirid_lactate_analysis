{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import ensemble, linear_model, metrics, model_selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Exploration of Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_max_pad = 10\n",
    "\n",
    "os.makedirs(\"./results/stats\", exist_ok=True)\n",
    "categorical_vars = []\n",
    "for i in range(HYPER_max_pad):\n",
    "    categorical_vars.extend(\n",
    "        [\n",
    "            f\"{x}_{i}\"\n",
    "            for x in (\n",
    "                \"Glasgow Coma Score - Verbal Response\",\n",
    "                \"Glasgow Coma Score - Motor Response\",\n",
    "                \"Glasgow Coma Score - Eye Opening\",\n",
    "                \"Glasgow Coma Score - Total\",\n",
    "                \"Circadian rhythm\",\n",
    "                \"Richmond agitation-sedation scale\",\n",
    "                \"Ventilator Airway Code\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "models = (\n",
    "    (\n",
    "        \"Logistic Regression\",\n",
    "        linear_model.LogisticRegression(random_state=666, n_jobs=-1),\n",
    "    ),\n",
    "    (\"Random Forest\", ensemble.RandomForestClassifier(random_state=666, n_jobs=-1)),\n",
    "    (\"Gradient Boosting\", ensemble.GradientBoostingClassifier(random_state=666)),\n",
    ")\n",
    "\n",
    "# adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "for model_name, model in models:\n",
    "    for cl, cl_name in ((1, \"Normal\"), (2, \"Mild\"), (3, \"Severe\")):\n",
    "        print(f\"Model: {model_name} - Class: {cl_name}\")\n",
    "        X_train = pd.read_csv(\n",
    "            f\"./results/splits/X_train_{cl}_{cl_name}_padded_translated.csv\",\n",
    "            dtype={k: \"category\" for k in categorical_vars},\n",
    "        )\n",
    "        y_train = pd.read_csv(f\"./results/splits/y_train_{cl}_{cl_name}.csv\")\n",
    "        X_test = pd.read_csv(\n",
    "            f\"./results/splits/X_test_{cl}_{cl_name}_padded_translated.csv\",\n",
    "            dtype={k: \"category\" for k in categorical_vars},\n",
    "        )\n",
    "        y_test = pd.read_csv(f\"./results/splits/y_test_{cl}_{cl_name}.csv\")\n",
    "        X = X_train.append(other=X_test)\n",
    "        y = y_train.append(other=y_test)\n",
    "\n",
    "        # drop useless columns\n",
    "        cols_to_drop = list()\n",
    "        for col in X.columns:\n",
    "            if (\n",
    "                \"Patient ID\" in col  # remove all \"Patient ID_N\"\n",
    "                or \"Sequential ID\" in col  # remove all \"Sequential ID_N\"\n",
    "            ):\n",
    "                cols_to_drop.append(col)\n",
    "        X.drop(labels=cols_to_drop, axis=1, inplace=True)\n",
    "        X = pd.get_dummies(data=X)\n",
    "\n",
    "        cv = model_selection.StratifiedKFold(n_splits=5, random_state=666, shuffle=True)\n",
    "        tprs = list()\n",
    "        aucs = list()\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        for i, (train, validation) in enumerate(cv.split(X, y)):\n",
    "            model.fit(X=X.iloc[train], y=y.iloc[train].values.ravel())\n",
    "            viz = metrics.RocCurveDisplay.from_estimator(\n",
    "                estimator=model,\n",
    "                X=X.iloc[validation],\n",
    "                y=y.iloc[validation].values.ravel(),\n",
    "                name=f\"ROC fold {i}\",\n",
    "                pos_label=0,\n",
    "                alpha=0.5,\n",
    "                lw=2,\n",
    "                ax=ax,\n",
    "            )\n",
    "            interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            tprs.append(interp_tpr)\n",
    "            aucs.append(viz.roc_auc)\n",
    "\n",
    "        ax.plot(\n",
    "            [0, 1], [0, 1], linestyle=\"--\", lw=4, color=\"r\", label=\"Chance\", alpha=0.8\n",
    "        )\n",
    "\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        ax.plot(\n",
    "            mean_fpr,\n",
    "            mean_tpr,\n",
    "            color=\"b\",\n",
    "            label=f\"Mean ROC (AUC = {mean_auc:0.2f} $\\\\pm$ {std_auc:0.2f})\",\n",
    "            lw=4,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        std_tpr = np.std(tprs, axis=0)\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        ax.fill_between(\n",
    "            mean_fpr,\n",
    "            tprs_lower,\n",
    "            tprs_upper,\n",
    "            color=\"grey\",\n",
    "            alpha=0.2,\n",
    "            label=\"$\\\\pm$ 1 std. dev.\",\n",
    "        )\n",
    "\n",
    "        ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "        ax.set_title(f\"Model: {model_name} - Class: {cl_name}\", fontsize=35)\n",
    "        ax.set_xlabel(\"1 - Specificity (FPR) (Positive label: 0)\", fontsize=27)\n",
    "        ax.set_ylabel(\"Sensitivity (TPR) (Positive label: 0)\", fontsize=27)\n",
    "        ax.set_xticklabels(labels=[x / 10 for x in range(0, 12, 2)], fontsize=25)\n",
    "        ax.set_yticklabels(labels=[x / 10 for x in range(0, 12, 2)], fontsize=25)\n",
    "        ax.legend(loc=\"lower right\", fontsize=25)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"./results/stats/{model_name}_{cl}_{cl_name}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning HyperParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_max_pad = 10\n",
    "\n",
    "categorical_vars = []\n",
    "for i in range(HYPER_max_pad):\n",
    "    categorical_vars.extend(\n",
    "        [\n",
    "            f\"{x}_{i}\"\n",
    "            for x in (\n",
    "                \"Glasgow Coma Score - Verbal Response\",\n",
    "                \"Glasgow Coma Score - Motor Response\",\n",
    "                \"Glasgow Coma Score - Eye Opening\",\n",
    "                \"Glasgow Coma Score - Total\",\n",
    "                \"Circadian rhythm\",\n",
    "                \"Richmond agitation-sedation scale\",\n",
    "                \"Ventilator Airway Code\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "models = (\n",
    "    (\n",
    "        \"Logistic Regression\",\n",
    "        linear_model.LogisticRegression(),\n",
    "        dict(\n",
    "            penalty=[\"l2\", \"none\"],\n",
    "            class_weight=[None, \"balanced\"],\n",
    "            random_state=[666],\n",
    "            max_iter=[100, 3000, 10000],\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        ensemble.RandomForestClassifier(),\n",
    "        dict(\n",
    "            n_estimators=[100, 500, 2500],\n",
    "            criterion=[\"gini\", \"entropy\"],\n",
    "            oob_score=[True, False],\n",
    "            class_weight=[None, \"balanced\", \"balanced_subsample\"],\n",
    "            random_state=[666],\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Gradient Boosting\",\n",
    "        ensemble.GradientBoostingClassifier(),\n",
    "        dict(\n",
    "            learning_rate=[0.3, 0.1, 0.05],\n",
    "            n_estimators=[100, 500, 2500],\n",
    "            max_depth=[3, 7, 15],\n",
    "            random_state=[666],\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = dict()\n",
    "for model_name, model, hyperparameters in models:\n",
    "    for cl, cl_name in ((1, \"Normal\"), (2, \"Mild\"), (3, \"Severe\")):\n",
    "        print(f\"Model: {model_name} - Class: {cl_name}\")\n",
    "        X_train = pd.read_csv(\n",
    "            f\"./results/splits/X_train_{cl}_{cl_name}_padded_translated.csv\",\n",
    "            dtype={k: \"category\" for k in categorical_vars},\n",
    "        )\n",
    "        y_train = pd.read_csv(f\"./results/splits/y_train_{cl}_{cl_name}.csv\")\n",
    "        X_test = pd.read_csv(\n",
    "            f\"./results/splits/X_test_{cl}_{cl_name}_padded_translated.csv\",\n",
    "            dtype={k: \"category\" for k in categorical_vars},\n",
    "        )\n",
    "        y_test = pd.read_csv(f\"./results/splits/y_test_{cl}_{cl_name}.csv\")\n",
    "\n",
    "        # drop useless columns\n",
    "        cols_to_drop = list()\n",
    "        for col in X_train.columns:\n",
    "            if (\n",
    "                \"Patient ID\" in col  # remove all \"Patient ID_N\"\n",
    "                or \"Sequential ID\" in col  # remove all \"Sequential ID_N\"\n",
    "            ):\n",
    "                cols_to_drop.append(col)\n",
    "        X_train.drop(labels=cols_to_drop, axis=1, inplace=True)\n",
    "        X_test.drop(labels=cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "        cv = model_selection.StratifiedKFold(n_splits=5, random_state=666, shuffle=True)\n",
    "        clf = model_selection.GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=hyperparameters,\n",
    "            scoring=\"roc_auc\",\n",
    "            n_jobs=-1,\n",
    "            cv=cv,\n",
    "        )\n",
    "        results[f\"{model_name}_{cl}_{cl_name}\"] = clf.fit(\n",
    "            X=X_train, y=y_train.values.ravel()\n",
    "        )\n",
    "\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}:\\n\\tBest score: {v.best_score_}\\n\\tBest parameters: {v.best_params_}\")\n",
    "\n",
    "# save some data of the hyperparameters' search\n",
    "with open(\"./results/best_models.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            k: dict(\n",
    "                param_grid=v.param_grid,\n",
    "                best_index=int(v.best_index_),\n",
    "                best_score_=float(v.best_score_),\n",
    "                best_params_=v.best_params_,\n",
    "                mean_fit_time=[float(x) for x in v.cv_results_[\"mean_fit_time\"]],\n",
    "                std_fit_time=[float(x) for x in v.cv_results_[\"std_fit_time\"]],\n",
    "                std_score_time=[float(x) for x in v.cv_results_[\"std_score_time\"]],\n",
    "                params=v.cv_results_[\"params\"],\n",
    "                split0_test_score=[\n",
    "                    float(x) for x in v.cv_results_[\"split0_test_score\"]\n",
    "                ],\n",
    "                split1_test_score=[\n",
    "                    float(x) for x in v.cv_results_[\"split1_test_score\"]\n",
    "                ],\n",
    "                split2_test_score=[\n",
    "                    float(x) for x in v.cv_results_[\"split2_test_score\"]\n",
    "                ],\n",
    "                split3_test_score=[\n",
    "                    float(x) for x in v.cv_results_[\"split3_test_score\"]\n",
    "                ],\n",
    "                split4_test_score=[\n",
    "                    float(x) for x in v.cv_results_[\"split4_test_score\"]\n",
    "                ],\n",
    "                mean_test_score=[float(x) for x in v.cv_results_[\"mean_test_score\"]],\n",
    "                std_test_score=[float(x) for x in v.cv_results_[\"std_test_score\"]],\n",
    "                rank_test_score=[float(x) for x in v.cv_results_[\"rank_test_score\"]],\n",
    "            )\n",
    "            for k, v in results.items()\n",
    "        },\n",
    "        f,\n",
    "        indent=4,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Best Model per Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal\n",
      "\tLogistic Regression_1_Normal: 0.7513760627755849\n",
      "\tParameters: {'class_weight': 'balanced', 'max_iter': 10000, 'penalty': 'none', 'random_state': 666}\n",
      "\tRandom Forest_1_Normal: 0.7886930500172323\n",
      "\tParameters: {'class_weight': None, 'criterion': 'entropy', 'n_estimators': 2500, 'oob_score': True, 'random_state': 666}\n",
      "\tGradient Boosting_1_Normal: 0.7794723182216481\n",
      "\tParameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'random_state': 666}\n",
      "Mild\n",
      "\tLogistic Regression_2_Mild: 0.7558701644531927\n",
      "\tParameters: {'class_weight': None, 'max_iter': 10000, 'penalty': 'l2', 'random_state': 666}\n",
      "\tRandom Forest_2_Mild: 0.7742036373876179\n",
      "\tParameters: {'class_weight': 'balanced', 'criterion': 'entropy', 'n_estimators': 2500, 'oob_score': True, 'random_state': 666}\n",
      "\tGradient Boosting_2_Mild: 0.7838918262082809\n",
      "\tParameters: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'random_state': 666}\n",
      "Severe\n",
      "\tLogistic Regression_3_Severe: 0.8453162302982408\n",
      "\tParameters: {'class_weight': None, 'max_iter': 10000, 'penalty': 'none', 'random_state': 666}\n",
      "\tRandom Forest_3_Severe: 0.8587469070317922\n",
      "\tParameters: {'class_weight': None, 'criterion': 'entropy', 'n_estimators': 2500, 'oob_score': True, 'random_state': 666}\n",
      "\tGradient Boosting_3_Severe: 0.870726327214409\n",
      "\tParameters: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500, 'random_state': 666}\n"
     ]
    }
   ],
   "source": [
    "best_models = None\n",
    "with open(\"./results/best_models.json\", \"r\") as f:\n",
    "    best_models = json.load(f)\n",
    "\n",
    "models = dict()\n",
    "for cl in (\"Normal\", \"Mild\", \"Severe\"):\n",
    "    print(cl)\n",
    "    for k, v in filter(lambda k: cl in k[0], best_models.items()):\n",
    "        print(f\"\\t{k}: {v['best_score_']}\\n\\tParameters: {v['best_params_']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_max_pad = 10\n",
    "\n",
    "os.makedirs(\"./results/stats\", exist_ok=True)\n",
    "categorical_vars = []\n",
    "for i in range(HYPER_max_pad):\n",
    "    categorical_vars.extend(\n",
    "        [\n",
    "            f\"{x}_{i}\"\n",
    "            for x in (\n",
    "                \"Glasgow Coma Score - Verbal Response\",\n",
    "                \"Glasgow Coma Score - Motor Response\",\n",
    "                \"Glasgow Coma Score - Eye Opening\",\n",
    "                \"Glasgow Coma Score - Total\",\n",
    "                \"Circadian rhythm\",\n",
    "                \"Richmond agitation-sedation scale\",\n",
    "                \"Ventilator Airway Code\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "best_models = None\n",
    "with open(\"./results/best_models.json\", \"r\") as f:\n",
    "    best_models = json.load(f)\n",
    "\n",
    "models = (\n",
    "    (\n",
    "        \"Gradient Boosting_1_Normal\",\n",
    "        ensemble.RandomForestClassifier(\n",
    "            **best_models[\"Gradient Boosting_1_Normal\"][\"best_params_\"], n_jobs=-1\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Gradient Boosting_2_Mild\",\n",
    "        ensemble.GradientBoostingClassifier(\n",
    "            **best_models[\"Gradient Boosting_2_Mild\"][\"best_params_\"]\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Gradient Boosting_3_Severe\",\n",
    "        ensemble.GradientBoostingClassifier(\n",
    "            **best_models[\"Gradient Boosting_3_Severe\"][\"best_params_\"]\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "stats_columns = [\n",
    "    \"Model_Class\",\n",
    "    \"Iteration\",\n",
    "    \"#True Positives\",\n",
    "    \"#False Negatives\",\n",
    "    \"#False Positives\",\n",
    "    \"#True Negatives\",\n",
    "    \"False Positive Rates\",\n",
    "    \"True Positive Rates\",\n",
    "    \"Interpolation FPR-TPR\",\n",
    "    \"ROC-AUC\",\n",
    "    \"No Skill\",\n",
    "    \"Precisions\",\n",
    "    \"Recalls\",\n",
    "    \"Interpolation P-R\",\n",
    "    \"PR-AUC\",\n",
    "]\n",
    "df_metrics = pd.DataFrame(columns=stats_columns)\n",
    "# adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "for model_name, model in models:\n",
    "    model_name, cl, cl_name = model_name.split(\"_\")\n",
    "    print(f\"Model: {model_name} - Class: {cl_name}\")\n",
    "    X_train = pd.read_csv(\n",
    "        f\"./results/splits/X_train_{cl}_{cl_name}_padded_translated.csv\",\n",
    "        dtype={k: \"category\" for k in categorical_vars},\n",
    "    )\n",
    "    y_train = pd.read_csv(f\"./results/splits/y_train_{cl}_{cl_name}.csv\")\n",
    "\n",
    "    # drop useless columns\n",
    "    cols_to_drop = list()\n",
    "    for col in X_train.columns:\n",
    "        if (\n",
    "            \"Patient ID\" in col  # remove all \"Patient ID_N\"\n",
    "            or \"Sequential ID\" in col  # remove all \"Sequential ID_N\"\n",
    "        ):\n",
    "            cols_to_drop.append(col)\n",
    "    X_train.drop(labels=cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    cv = model_selection.StratifiedKFold(n_splits=5, random_state=666, shuffle=True)\n",
    "    fig1, ax1 = plt.subplots(figsize=(20, 10))\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 10))\n",
    "    fig3, ax3 = plt.subplots(figsize=(20, 10))\n",
    "    for i, (train, validation) in enumerate(cv.split(X_train, y_train)):\n",
    "        model.fit(X=X_train.iloc[train], y=y_train.iloc[train].values.ravel())\n",
    "        cm_display = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "            estimator=model,\n",
    "            X=X_train.iloc[validation],\n",
    "            y=y_train.iloc[validation].values.ravel(),\n",
    "            ax=ax1,\n",
    "        )\n",
    "        pr_display = metrics.PrecisionRecallDisplay.from_estimator(\n",
    "            estimator=model,\n",
    "            X=X_train.iloc[validation],\n",
    "            y=y_train.iloc[validation].values.ravel(),\n",
    "            name=f\"PR fold {i}\",\n",
    "            pos_label=0,\n",
    "            alpha=0.5,\n",
    "            lw=2,\n",
    "            ax=ax2,\n",
    "        )\n",
    "        roc_display = metrics.RocCurveDisplay.from_estimator(\n",
    "            estimator=model,\n",
    "            X=X_train.iloc[validation],\n",
    "            y=y_train.iloc[validation].values.ravel(),\n",
    "            name=f\"ROC fold {i}\",\n",
    "            pos_label=0,\n",
    "            alpha=0.5,\n",
    "            lw=2,\n",
    "            ax=ax3,\n",
    "        )\n",
    "        interpolated_fpr_tpr = np.interp(\n",
    "            np.linspace(0, 1, 100),  # interpolation range\n",
    "            roc_display.fpr,  # x\n",
    "            roc_display.tpr,  # y\n",
    "        )\n",
    "        interpolated_fpr_tpr[0] = 0.0\n",
    "        interpolated_pr_rec = np.interp(\n",
    "            np.linspace(0, 1, 100),  # interpolation range\n",
    "            pr_display.precision,  # x\n",
    "            pr_display.recall,  # y\n",
    "        )\n",
    "        interpolated_pr_rec[0] = 0.0\n",
    "        df_metrics = df_metrics.append(\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        f\"{model_name}_{cl}_{cl_name}\",  # model_class_class_name\n",
    "                        i,  # iteration\n",
    "                        cm_display.confusion_matrix[0][0],  # tp\n",
    "                        cm_display.confusion_matrix[0][1],  # fn\n",
    "                        cm_display.confusion_matrix[1][0],  # fp\n",
    "                        cm_display.confusion_matrix[1][1],  # tn\n",
    "                        \",\".join(\n",
    "                            f\"{float(x)}\" for x in roc_display.fpr\n",
    "                        ),  # comma-separated values\n",
    "                        \",\".join(\n",
    "                            f\"{float(x)}\" for x in roc_display.tpr\n",
    "                        ),  # comma-separated values\n",
    "                        \",\".join(\n",
    "                            f\"{float(x)}\" for x in interpolated_fpr_tpr\n",
    "                        ),  # comma-separated values\n",
    "                        roc_display.roc_auc,  # roc auc\n",
    "                        len(y_train.iloc[validation][y_train[\"Lactate_Outcome\"] == 1])\n",
    "                        / len(y_train.iloc[validation]),  # no skill\n",
    "                        \",\".join(\n",
    "                            f\"{float(x)}\" for x in pr_display.precision\n",
    "                        ),  # comma-separated values\n",
    "                        \",\".join(\n",
    "                            f\"{float(x)}\" for x in pr_display.recall\n",
    "                        ),  # comma-separated values\n",
    "                        \",\".join(\n",
    "                            f\"{float(x)}\" for x in interpolated_pr_rec\n",
    "                        ),  # comma-separated values\n",
    "                        pr_display.average_precision,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=stats_columns,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    tprs = [\n",
    "        [float(tpr) for tpr in tprs_i.split(\",\")]\n",
    "        for tprs_i in df_metrics[\n",
    "            df_metrics[\"Model_Class\"] == f\"{model_name}_{cl}_{cl_name}\"\n",
    "        ][\"Interpolation FPR-TPR\"]\n",
    "    ]\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = metrics.auc(np.linspace(0, 1, 100), mean_tpr)\n",
    "    rocs = [\n",
    "        roc_auc\n",
    "        for roc_auc in df_metrics[\n",
    "            df_metrics[\"Model_Class\"] == f\"{model_name}_{cl}_{cl_name}\"\n",
    "        ][\"ROC-AUC\"]\n",
    "    ]\n",
    "    std_auc = np.std(rocs)\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "    ax3.plot([0, 1], [0, 1], linestyle=\"--\", lw=4, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "    ax3.plot(\n",
    "        np.linspace(0, 1, 100),\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=f\"Mean ROC (AUC = {mean_auc:0.2f} $\\\\pm$ {std_auc:0.2f})\",\n",
    "        lw=4,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    ax3.fill_between(\n",
    "        np.linspace(0, 1, 100),\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=\"$\\\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax3.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "    ax3.set_title(f\"Model: {model_name} - Class: {cl_name}\", fontsize=35)\n",
    "    ax3.set_xlabel(\"1 - Specificity (FPR) (Positive label: 0)\", fontsize=27)\n",
    "    ax3.set_ylabel(\"Sensitivity (TPR) (Positive label: 0)\", fontsize=27)\n",
    "    ax3.set_xticklabels(labels=[x / 10 for x in range(0, 12, 2)], fontsize=25)\n",
    "    ax3.set_yticklabels(labels=[x / 10 for x in range(0, 12, 2)], fontsize=25)\n",
    "    ax3.legend(loc=\"lower right\", fontsize=25)\n",
    "    plt.tight_layout()\n",
    "    fig2.savefig(f\"./results/stats/tuned_{model_name}_{cl}_{cl_name}_prauc.png\")\n",
    "    fig3.savefig(f\"./results/stats/tuned_{model_name}_{cl}_{cl_name}_rocauc.png\")\n",
    "    plt.close()\n",
    "\n",
    "df_metrics[\"Accuracy\"] = (\n",
    "    df_metrics[\"#True Positives\"] + df_metrics[\"#True Negatives\"]\n",
    ") / (\n",
    "    df_metrics[\"#True Positives\"]\n",
    "    + df_metrics[\"#True Negatives\"]\n",
    "    + df_metrics[\"#False Positives\"]\n",
    "    + df_metrics[\"#False Negatives\"]\n",
    ").replace(\n",
    "    {0: np.nan}\n",
    ")\n",
    "# positive predictive value = precision\n",
    "df_metrics[\"Positive Predictive Value\"] = df_metrics[\"#True Positives\"] / (\n",
    "    df_metrics[\"#True Positives\"] + df_metrics[\"#False Positives\"]\n",
    ").replace({0: np.nan})\n",
    "df_metrics[\"Negative Predictive Value\"] = df_metrics[\"#True Negatives\"] / (\n",
    "    df_metrics[\"#True Negatives\"] + df_metrics[\"#False Negatives\"]\n",
    ").replace({0: np.nan})\n",
    "# sensitivity = recall\n",
    "df_metrics[\"Sensitivity\"] = df_metrics[\"#True Positives\"] / (\n",
    "    df_metrics[\"#True Positives\"] + df_metrics[\"#False Negatives\"]\n",
    ").replace({0: np.nan})\n",
    "df_metrics[\"Specificity\"] = df_metrics[\"#True Negatives\"] / (\n",
    "    df_metrics[\"#True Negatives\"] + df_metrics[\"#False Positives\"]\n",
    ").replace({0: np.nan})\n",
    "df_metrics[\"F1\"] = (\n",
    "    2\n",
    "    * (df_metrics[\"Positive Predictive Value\"] * df_metrics[\"Sensitivity\"])\n",
    "    / (df_metrics[\"Positive Predictive Value\"] + df_metrics[\"Sensitivity\"]).replace(\n",
    "        {0: np.nan}\n",
    "    )\n",
    ")\n",
    "df_metrics[\"Matthews Correlation Coefficient\"] = (\n",
    "    (df_metrics[\"#True Positives\"] * df_metrics[\"#True Negatives\"])\n",
    "    - (df_metrics[\"#False Positives\"] * df_metrics[\"#False Negatives\"])\n",
    ") / (\n",
    "    (\n",
    "        (df_metrics[\"#True Positives\"] + df_metrics[\"#False Positives\"])\n",
    "        * (df_metrics[\"#True Positives\"] + df_metrics[\"#False Negatives\"])\n",
    "        * (df_metrics[\"#True Negatives\"] + df_metrics[\"#False Positives\"])\n",
    "        * (df_metrics[\"#True Negatives\"] + df_metrics[\"#False Negatives\"])\n",
    "    ).replace({0: np.nan})\n",
    "    ** (1 / 2)  # sqrt\n",
    ")\n",
    "df_metrics.to_csv(\"./results/stats/cv_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#True Positives: Mean: 2949.07, STD: 3475.07\n",
      "#False Negatives: Mean: 245.93, STD: 226.03\n",
      "#False Positives: Mean: 403.07, STD: 232.21\n",
      "#True Negatives: Mean: 927.33, STD: 707.69\n",
      "Error while computing mean and stdev for False Positive Rates\n",
      "Error while computing mean and stdev for True Positive Rates\n",
      "Error while computing mean and stdev for Interpolation FPR-TPR\n",
      "ROC-AUC: Mean: 0.81, STD: 0.04\n",
      "No Skill: Mean: 0.47, STD: 0.28\n",
      "Error while computing mean and stdev for Precisions\n",
      "Error while computing mean and stdev for Recalls\n",
      "Error while computing mean and stdev for Interpolation P-R\n",
      "PR-AUC: Mean: 0.78, STD: 0.14\n",
      "Accuracy: Mean: 0.82, STD: 0.08\n",
      "Positive Predictive Value: Mean: 0.75, STD: 0.12\n",
      "Negative Predictive Value: Mean: 0.73, STD: 0.23\n",
      "Sensitivity: Mean: 0.72, STD: 0.20\n",
      "Specificity: Mean: 0.57, STD: 0.40\n",
      "F1: Mean: 0.74, STD: 0.16\n",
      "Matthews Correlation Coefficient: Mean: 0.41, STD: 0.14\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.read_csv(\"./results/stats/cv_metrics.csv\")\n",
    "for col in df_metrics.columns[2:]:\n",
    "    try:\n",
    "        print(\n",
    "            f\"{col}: Mean: {np.nanmean(df_metrics[col]):0.2f}, STD: {np.nanstd(df_metrics[col]):0.2f}\"\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(f\"Error while computing mean and stdev for {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPER_max_pad = 10\n",
    "\n",
    "os.makedirs(\"./results/stats\", exist_ok=True)\n",
    "categorical_vars = []\n",
    "for i in range(HYPER_max_pad):\n",
    "    categorical_vars.extend(\n",
    "        [\n",
    "            f\"{x}_{i}\"\n",
    "            for x in (\n",
    "                \"Glasgow Coma Score - Verbal Response\",\n",
    "                \"Glasgow Coma Score - Motor Response\",\n",
    "                \"Glasgow Coma Score - Eye Opening\",\n",
    "                \"Glasgow Coma Score - Total\",\n",
    "                \"Circadian rhythm\",\n",
    "                \"Richmond agitation-sedation scale\",\n",
    "                \"Ventilator Airway Code\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "best_models = None\n",
    "with open(\"./results/best_models.json\", \"r\") as f:\n",
    "    best_models = json.load(f)\n",
    "\n",
    "models = (\n",
    "    (\n",
    "        \"Random Forest_1_Normal\",\n",
    "        ensemble.RandomForestClassifier(\n",
    "            **best_models[\"Random Forest_1_Normal\"][\"best_params_\"], n_jobs=-1\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Gradient Boosting_2_Mild\",\n",
    "        ensemble.GradientBoostingClassifier(\n",
    "            **best_models[\"Gradient Boosting_2_Mild\"][\"best_params_\"]\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Gradient Boosting_3_Severe\",\n",
    "        ensemble.GradientBoostingClassifier(\n",
    "            **best_models[\"Gradient Boosting_3_Severe\"][\"best_params_\"]\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "stats_columns = [\n",
    "    \"Model_Class\",\n",
    "    \"#True Positives\",\n",
    "    \"#False Negatives\",\n",
    "    \"#False Positives\",\n",
    "    \"#True Negatives\",\n",
    "    \"False Positive Rates\",\n",
    "    \"True Positive Rates\",\n",
    "    \"Interpolation FPR-TPR\",\n",
    "    \"ROC-AUC\",\n",
    "    \"No Skill\",\n",
    "    \"Precisions\",\n",
    "    \"Recalls\",\n",
    "    \"Interpolation P-R\",\n",
    "    \"PR-AUC\",\n",
    "]\n",
    "df_metrics = pd.DataFrame(columns=stats_columns)\n",
    "# adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "for model_name, model in models:\n",
    "    model_name, cl, cl_name = model_name.split(\"_\")\n",
    "    print(f\"Model: {model_name} - Class: {cl_name}\")\n",
    "    X_train = pd.read_csv(\n",
    "        f\"./results/splits/X_train_{cl}_{cl_name}_padded_translated.csv\",\n",
    "        dtype={k: \"category\" for k in categorical_vars},\n",
    "    )\n",
    "    y_train = pd.read_csv(f\"./results/splits/y_train_{cl}_{cl_name}.csv\")\n",
    "    X_test = pd.read_csv(\n",
    "        f\"./results/splits/X_test_{cl}_{cl_name}_padded_translated.csv\",\n",
    "        dtype={k: \"category\" for k in categorical_vars},\n",
    "    )\n",
    "    y_test = pd.read_csv(f\"./results/splits/y_test_{cl}_{cl_name}.csv\")\n",
    "\n",
    "    # drop useless columns\n",
    "    cols_to_drop = list()\n",
    "    for col in X_train.columns:\n",
    "        if (\n",
    "            \"Patient ID\" in col  # remove all \"Patient ID_N\"\n",
    "            or \"Sequential ID\" in col  # remove all \"Sequential ID_N\"\n",
    "        ):\n",
    "            cols_to_drop.append(col)\n",
    "    X_train.drop(labels=cols_to_drop, axis=1, inplace=True)\n",
    "    X_test.drop(labels=cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    fig1, ax1 = plt.subplots(figsize=(20, 10))\n",
    "    fig2, ax2 = plt.subplots(figsize=(20, 10))\n",
    "    fig3, ax3 = plt.subplots(figsize=(20, 10))\n",
    "    model.fit(X=X_train, y=y_train.values.ravel())\n",
    "    cm_display = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "        estimator=model, X=X_test, y=y_test, ax=ax1\n",
    "    )\n",
    "    pr_display = metrics.PrecisionRecallDisplay.from_estimator(\n",
    "        estimator=model, X=X_test, y=y_test, name=f\"PR\", pos_label=0, ax=ax2\n",
    "    )\n",
    "    roc_display = metrics.RocCurveDisplay.from_estimator(\n",
    "        estimator=model, X=X_test, y=y_test, name=f\"ROC\", pos_label=0, ax=ax3\n",
    "    )\n",
    "    interpolated_fpr_tpr = np.interp(\n",
    "        np.linspace(0, 1, 100),  # interpolation range\n",
    "        roc_display.fpr,  # x\n",
    "        roc_display.tpr,  # y\n",
    "    )\n",
    "    interpolated_fpr_tpr[0] = 0.0\n",
    "    interpolated_pr_rec = np.interp(\n",
    "        np.linspace(0, 1, 100),  # interpolation range\n",
    "        pr_display.precision,  # x\n",
    "        pr_display.recall,  # y\n",
    "    )\n",
    "    interpolated_pr_rec[0] = 0.0\n",
    "    df_metrics = df_metrics.append(\n",
    "        pd.DataFrame(\n",
    "            [\n",
    "                [\n",
    "                    f\"{model_name}_{cl}_{cl_name}\",  # model_class_class_name\n",
    "                    cm_display.confusion_matrix[0][0],  # tp\n",
    "                    cm_display.confusion_matrix[0][1],  # fn\n",
    "                    cm_display.confusion_matrix[1][0],  # fp\n",
    "                    cm_display.confusion_matrix[1][1],  # tn\n",
    "                    \",\".join(\n",
    "                        f\"{float(x)}\" for x in roc_display.fpr\n",
    "                    ),  # comma-separated values\n",
    "                    \",\".join(\n",
    "                        f\"{float(x)}\" for x in roc_display.tpr\n",
    "                    ),  # comma-separated values\n",
    "                    \",\".join(\n",
    "                        f\"{float(x)}\" for x in interpolated_fpr_tpr\n",
    "                    ),  # comma-separated values\n",
    "                    roc_display.roc_auc,  # roc auc\n",
    "                    len(y_test[y_test[\"Lactate_Outcome\"] == 1])\n",
    "                    / len(y_test),  # no skill\n",
    "                    \",\".join(\n",
    "                        f\"{float(x)}\" for x in pr_display.precision\n",
    "                    ),  # comma-separated values\n",
    "                    \",\".join(\n",
    "                        f\"{float(x)}\" for x in pr_display.recall\n",
    "                    ),  # comma-separated values\n",
    "                    \",\".join(\n",
    "                        f\"{float(x)}\" for x in interpolated_pr_rec\n",
    "                    ),  # comma-separated values\n",
    "                    pr_display.average_precision,\n",
    "                ]\n",
    "            ],\n",
    "            columns=stats_columns,\n",
    "        )\n",
    "    )\n",
    "    ax3.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    ax3.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "    ax3.set_title(f\"Model: {model_name} - Class: {cl_name}\", fontsize=35)\n",
    "    ax3.set_xlabel(\"1 - Specificity (FPR) (Positive label: 0)\", fontsize=27)\n",
    "    ax3.set_ylabel(\"Sensitivity (TPR) (Positive label: 0)\", fontsize=27)\n",
    "    ax3.set_xticklabels(labels=[x / 10 for x in range(0, 12, 2)], fontsize=25)\n",
    "    ax3.set_yticklabels(labels=[x / 10 for x in range(0, 12, 2)], fontsize=25)\n",
    "    ax3.legend(loc=\"lower right\", fontsize=25)\n",
    "    plt.tight_layout()\n",
    "    fig1.savefig(f\"./results/stats/final_{model_name}_{cl}_{cl_name}_confmat.png\")\n",
    "    fig2.savefig(f\"./results/stats/final_{model_name}_{cl}_{cl_name}_prauc.png\")\n",
    "    fig3.savefig(f\"./results/stats/final_{model_name}_{cl}_{cl_name}_rocauc.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "df_metrics[\"Accuracy\"] = (\n",
    "    df_metrics[\"#True Positives\"] + df_metrics[\"#True Negatives\"]\n",
    ") / (\n",
    "    df_metrics[\"#True Positives\"]\n",
    "    + df_metrics[\"#True Negatives\"]\n",
    "    + df_metrics[\"#False Positives\"]\n",
    "    + df_metrics[\"#False Negatives\"]\n",
    ")\n",
    "# positive predictive value = precision\n",
    "df_metrics[\"Positive Predictive Value\"] = df_metrics[\"#True Positives\"] / (\n",
    "    df_metrics[\"#True Positives\"] + df_metrics[\"#False Positives\"]\n",
    ")\n",
    "df_metrics[\"Negative Predictive Value\"] = df_metrics[\"#True Negatives\"] / (\n",
    "    df_metrics[\"#True Negatives\"] + df_metrics[\"#False Negatives\"]\n",
    ")\n",
    "# sensitivity = recall\n",
    "df_metrics[\"Sensitivity\"] = df_metrics[\"#True Positives\"] / (\n",
    "    df_metrics[\"#True Positives\"] + df_metrics[\"#False Negatives\"]\n",
    ")\n",
    "df_metrics[\"Specificity\"] = df_metrics[\"#True Negatives\"] / (\n",
    "    df_metrics[\"#True Negatives\"] + df_metrics[\"#False Positives\"]\n",
    ")\n",
    "df_metrics[\"F1\"] = (\n",
    "    2\n",
    "    * (df_metrics[\"Positive Predictive Value\"] * df_metrics[\"Sensitivity\"])\n",
    "    / (df_metrics[\"Positive Predictive Value\"] + df_metrics[\"Sensitivity\"])\n",
    ")\n",
    "df_metrics[\"Matthews Correlation Coefficient\"] = (\n",
    "    (df_metrics[\"#True Positives\"] * df_metrics[\"#True Negatives\"])\n",
    "    - (df_metrics[\"#False Positives\"] * df_metrics[\"#False Negatives\"])\n",
    ") / (\n",
    "    (\n",
    "        (df_metrics[\"#True Positives\"] + df_metrics[\"#False Positives\"])\n",
    "        * (df_metrics[\"#True Positives\"] + df_metrics[\"#False Negatives\"])\n",
    "        * (df_metrics[\"#True Negatives\"] + df_metrics[\"#False Positives\"])\n",
    "        * (df_metrics[\"#True Negatives\"] + df_metrics[\"#False Negatives\"])\n",
    "    )\n",
    "    ** (1 / 2)  # sqrt\n",
    ")\n",
    "df_metrics.to_csv(\"./results/stats/final_metrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#True Positives: Mean: 2949.07, STD: 3475.07\n",
      "#False Negatives: Mean: 245.93, STD: 226.03\n",
      "#False Positives: Mean: 403.07, STD: 232.21\n",
      "#True Negatives: Mean: 927.33, STD: 707.69\n",
      "Error while computing mean and stdev for False Positive Rates\n",
      "Error while computing mean and stdev for True Positive Rates\n",
      "Error while computing mean and stdev for Interpolation FPR-TPR\n",
      "ROC-AUC: Mean: 0.81, STD: 0.04\n",
      "No Skill: Mean: 0.47, STD: 0.28\n",
      "Error while computing mean and stdev for Precisions\n",
      "Error while computing mean and stdev for Recalls\n",
      "Error while computing mean and stdev for Interpolation P-R\n",
      "PR-AUC: Mean: 0.78, STD: 0.14\n",
      "Accuracy: Mean: 0.82, STD: 0.08\n",
      "Positive Predictive Value: Mean: 0.75, STD: 0.12\n",
      "Negative Predictive Value: Mean: 0.73, STD: 0.23\n",
      "Sensitivity: Mean: 0.72, STD: 0.20\n",
      "Specificity: Mean: 0.57, STD: 0.40\n",
      "F1: Mean: 0.74, STD: 0.16\n",
      "Matthews Correlation Coefficient: Mean: 0.41, STD: 0.14\n"
     ]
    }
   ],
   "source": [
    "df_metrics = pd.read_csv(\"./results/stats/final_metrics.csv\")\n",
    "for col in df_metrics.columns[1:]:\n",
    "    try:\n",
    "        print(\n",
    "            f\"{col}: Mean: {np.nanmean(df_metrics[col]):0.2f}, STD: {np.nanstd(df_metrics[col]):0.2f}\"\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(f\"Error while computing mean and stdev for {col}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c643c1ae580a7eadaaaf8eb2846b79a7c1c6576dfd78895ee4877fd48cad0027"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
